{"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"8228d6a5132748cea194d68352611f7b":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_acdab782ba61489ba8b7f077e7c24901","IPY_MODEL_8242c4d228ee4718bf7a1f651baf04f3","IPY_MODEL_f88a7c00a662488eb1d77a88ccfabe87"],"layout":"IPY_MODEL_3995bab3430443e7897939f778c9c117"}},"acdab782ba61489ba8b7f077e7c24901":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_2050b090460645449e8a838fd69d8180","placeholder":"​","style":"IPY_MODEL_3e48366fb7984b5e8493424fbb7dfb4f","value":"config.json: 100%"}},"8242c4d228ee4718bf7a1f651baf04f3":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_0eb315b1dc4040e2a227c8523ec6c6be","max":70043,"min":0,"orientation":"horizontal","style":"IPY_MODEL_4844c30ede1349d7b21a8e839e7d153c","value":70043}},"f88a7c00a662488eb1d77a88ccfabe87":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_1cef731d0ba04ffe8172c50d378dc141","placeholder":"​","style":"IPY_MODEL_f45f9409fab44d9581d0f6bb59399d08","value":" 70.0k/70.0k [00:00&lt;00:00, 2.79MB/s]"}},"3995bab3430443e7897939f778c9c117":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2050b090460645449e8a838fd69d8180":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3e48366fb7984b5e8493424fbb7dfb4f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"0eb315b1dc4040e2a227c8523ec6c6be":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4844c30ede1349d7b21a8e839e7d153c":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"1cef731d0ba04ffe8172c50d378dc141":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f45f9409fab44d9581d0f6bb59399d08":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"1ffc5523658b462ebbe0afc23b010f80":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_b74b22071cf14395bea91d599c068fff","IPY_MODEL_d86c21ae99f4460ca9c51be1d39c4ce4","IPY_MODEL_6cbf2f1c833f4a30ba81d27f4030fd4f"],"layout":"IPY_MODEL_aa93b8528e4842e595c8a791c425ec45"}},"b74b22071cf14395bea91d599c068fff":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_897fa3d296784b538a3654c8d59398ec","placeholder":"​","style":"IPY_MODEL_62b8da8bbe0f4fe8b8ce80898ea4f9e9","value":"tf_model.h5: 100%"}},"d86c21ae99f4460ca9c51be1d39c4ce4":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_d86813f118334a079cf7677777853309","max":14563152,"min":0,"orientation":"horizontal","style":"IPY_MODEL_79fc8e20ca4e45b6b98db1e6b207a966","value":14563152}},"6cbf2f1c833f4a30ba81d27f4030fd4f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_8e000d8e1b9e46549ee855c0cb0abb61","placeholder":"​","style":"IPY_MODEL_0481570fc12c4a498692a2b86e637f63","value":" 14.6M/14.6M [00:00&lt;00:00, 94.9MB/s]"}},"aa93b8528e4842e595c8a791c425ec45":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"897fa3d296784b538a3654c8d59398ec":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"62b8da8bbe0f4fe8b8ce80898ea4f9e9":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d86813f118334a079cf7677777853309":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"79fc8e20ca4e45b6b98db1e6b207a966":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"8e000d8e1b9e46549ee855c0cb0abb61":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0481570fc12c4a498692a2b86e637f63":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"08a24be5f056435ca1b164f51533add6":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_4b5f8ef0e10f4d1f9664a1dbe8eb0855","IPY_MODEL_c64e6eabe352442783b132aeeb558b75","IPY_MODEL_0aef7524cf034224bd73131a65407aa5"],"layout":"IPY_MODEL_06191f4b2ad54e96992d3d1680f96560"}},"4b5f8ef0e10f4d1f9664a1dbe8eb0855":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b7ffe49e2c9c4dc48f99b0f49850d3b4","placeholder":"​","style":"IPY_MODEL_e391195cf0ce4232aff39e6facb13ea2","value":"tf_model.h5: 100%"}},"c64e6eabe352442783b132aeeb558b75":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_eabe7613fd8c48609a55d8ff29f8ad5d","max":15158640,"min":0,"orientation":"horizontal","style":"IPY_MODEL_b456f9bfb18d48caba2466fdc6df287b","value":15158640}},"0aef7524cf034224bd73131a65407aa5":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7571cd01e0ed4aae9645fe97a24c67db","placeholder":"​","style":"IPY_MODEL_7486786f6dd2404486c68763bde0123a","value":" 15.2M/15.2M [00:01&lt;00:00, 21.4MB/s]"}},"06191f4b2ad54e96992d3d1680f96560":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b7ffe49e2c9c4dc48f99b0f49850d3b4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e391195cf0ce4232aff39e6facb13ea2":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"eabe7613fd8c48609a55d8ff29f8ad5d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b456f9bfb18d48caba2466fdc6df287b":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"7571cd01e0ed4aae9645fe97a24c67db":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7486786f6dd2404486c68763bde0123a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":7541023,"sourceType":"datasetVersion","datasetId":4391289},{"sourceId":7655615,"sourceType":"datasetVersion","datasetId":4463392},{"sourceId":7676571,"sourceType":"datasetVersion","datasetId":4478068}],"dockerImageVersionId":30646,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"#@title **Install & Import**\n!pip install tensorflow_io\n!!pip install transformers -q\n\nimport tensorflow as tf\nimport tensorflow_io as tfio\nimport os\nimport glob\nimport matplotlib.pyplot as plt\nimport tifffile\nfrom tqdm import tqdm\nimport pickle\nfrom transformers import TFSegformerForSemanticSegmentation\nimport pandas as pd\n\nfrom PIL import Image\nimport cv2\nimport numpy as np\n\ntf.config.list_physical_devices('GPU') #Check GPU\n# from google.colab import drive #grive\n# drive.mount('/content/drive', force_remount=True)","metadata":{"id":"SOpVWqwY7MM5","outputId":"471b1e8e-92cb-4491-ae97-42e01715d203","execution":{"iopub.status.busy":"2024-02-22T07:49:24.113374Z","iopub.execute_input":"2024-02-22T07:49:24.113719Z","iopub.status.idle":"2024-02-22T07:50:05.156600Z","shell.execute_reply.started":"2024-02-22T07:49:24.113689Z","shell.execute_reply":"2024-02-22T07:50:05.155528Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"0 - VV\n\n1 - VH\n\n2 - Merit DEM\n\n3 - Cop DEM\n\n4 - Land cover\n\n5 - Water occurrence\n\n6 - VV/VH\n\n7 - DSM + LC\n\n8 - DT","metadata":{}},{"cell_type":"code","source":"batch_size = 8\nepochs = 50\nauto = tf.data.AUTOTUNE\n# Paths to your data directories\nimages_dir = r\"/kaggle/input/track1/Track1/train/images/\"\nlabels_dir = r\"/kaggle/input/track1/Track1/train/labels/\"\nimage_size = 512","metadata":{"id":"w_n_f1fa86CN","execution":{"iopub.status.busy":"2024-02-22T07:50:22.540360Z","iopub.execute_input":"2024-02-22T07:50:22.540705Z","iopub.status.idle":"2024-02-22T07:50:22.546058Z","shell.execute_reply.started":"2024-02-22T07:50:22.540678Z","shell.execute_reply":"2024-02-22T07:50:22.544275Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def parse_image(file_path):\n    image = tf.convert_to_tensor(np.array(tifffile.imread(file_path)))\n    return image\n\ndef parse_mask(file_path):\n    mask = tf.io.read_file(file_path)\n    mask = tf.image.decode_png(mask, channels=1)\n    mask = tf.image.resize(mask, [image_size, image_size], method='nearest')\n    mask = tf.squeeze(mask)  # Ensure it's a 2D tensor for the mask\n    mask.set_shape([image_size, image_size])\n    return mask\n\n# def load_data(image_path, mask_path):\n#     image = parse_image(image_path)\n#     mask = parse_mask(mask_path)\n#     image = tf.transpose(image, (2, 0, 1))  # Adjust dimensions if necessary\n#     return {\"pixel_values\": image, \"labels\": mask}\n\ndef load_and_preprocessdata(image_path, mask_path, mean, std_dev): #, \n    image = parse_image(image_path)\n    mask = parse_mask(mask_path)\n    image = tf.transpose(image, (2, 0, 1))  # Adjust dimensions if necessary\n\n    # preprocessing pipeline\n    image = np.concatenate([image, np.zeros_like(image[:3, :, :])], axis=0)\n    image = calc_vv_vh(image)    #add vv/vh\n    image = calc_dem_lc(image)  #add dem + lc\n    image = calc_dist_transform(image)    #add distance transform\n    image = calc_norm(image, mean, variance=std_dev**2, axis=0)   #normalize image\n\n    #drop features not required\n    channels_to_keep = [0,1,2,3,5,6,7,8] #dropping band 4 - land cover class- this info is captured in dem_lc. also doesn't make sense normalising categorical data \n    modified_image = tf.gather(image, channels_to_keep, axis=0)\n    return {\"pixel_values\": modified_image, \"labels\": mask}\n\ndef calc_vv_vh(image):\n    denominator = image[1, :, :]\n    image[6, :, :] = np.nan_to_num(np.divide(image[0, :, :], denominator), nan=0)\n    return image\n\ndef calc_dem_lc(image):\n    dem_quantile = np.where(image[2,:,:] <= np.quantile(image[2,:,:],0.1), 1, 0)\n    binary_image = np.where(image[4,:,:] >= 80, 1, 0)\n    image[7, :, :] = np.sum([dem_quantile, binary_image], axis=0)\n    return image\n\ndef calc_dist_transform(image):\n    #print(np.min(image[4, :, :]),np.max(image[4, :, :]) )\n    binary_image = np.where(image[4, :, :] >= 80, 0,1).astype(np.uint8)\n    #print(np.min(binary_image), np.max(binary_image))\n    dist = cv2.distanceTransform(binary_image, cv2.DIST_L2, 3)\n    image[8, :, :] = dist\n    return image\n\ndef calc_norm(image, mean, variance, axis):\n    layer = tf.keras.layers.Normalization(axis=axis, mean=mean, variance=variance, invert=False)\n    image = layer(image)\n    return image\n\n# def get_image_mask_paths(images_dir, labels_dir):\n#     image_paths = sorted([os.path.join(images_dir, fname) for fname in sorted(os.listdir(images_dir))])\n#     mask_paths = sorted([os.path.join(labels_dir, fname) for fname in sorted(os.listdir(labels_dir))])\n#     return image_paths, mask_paths\n\ndef get_image_mask_paths(images_dir, labels_dir, images_names, labels_names):\n    image_paths = sorted([os.path.join(images_dir, fname) for fname in sorted(images_names)])\n    mask_paths = sorted([os.path.join(labels_dir, fname) for fname in sorted(labels_names)])\n    return image_paths, mask_paths\n\n# def image_mask_generator(image_paths, mask_paths):\n#     for img_path, mask_path in zip(image_paths, mask_paths):\n#         yield load_data(img_path, mask_path)\n\ndef image_mask_generator(image_paths, mask_paths, mean, std_dev):\n    for img_path, mask_path in zip(image_paths, mask_paths):\n        yield load_and_preprocessdata(img_path, mask_path, mean, std_dev)\n\ndef prepare_for_training(ds, cache=True):\n    if cache:\n        if isinstance(cache, str):\n            ds = ds.cache(cache)\n        else:\n            ds = ds.cache()\n    # Repeat forever\n    ds = ds.repeat()\n    ds = ds.batch(batch_size)\n    ds = ds.prefetch(buffer_size=auto)\n    return ds\n\n# wrap up upsampling and argmax in a function\ndef upsampled_and_argmax(predictions):\n    upsampled = tf.keras.layers.UpSampling2D(size=(4,4), data_format=\"channels_first\",\n                                           interpolation='bilinear')(predictions.logits)\n    labels = np.argmax(upsampled, axis=1)\n    return labels","metadata":{"id":"biFhXZyt9MTM","execution":{"iopub.status.busy":"2024-02-20T11:47:52.610715Z","iopub.execute_input":"2024-02-20T11:47:52.611091Z","iopub.status.idle":"2024-02-20T11:47:52.633471Z","shell.execute_reply.started":"2024-02-20T11:47:52.611062Z","shell.execute_reply":"2024-02-20T11:47:52.632311Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# load the pickled data splits into lists\npath = r'/kaggle/input/train-test-split-and-norm-statistics/'\n\nwith open(path + 'train_label_filenames.pkl', 'rb') as f:\n    train_label_filenames = pickle.load(f)\n\nwith open(path+'train_feature_filenames.pkl', 'rb') as f:\n    train_feature_filenames = pickle.load(f)\n\nwith open(path + 'val_label_filenames.pkl', 'rb') as f:\n    val_label_filenames = pickle.load(f)\n\nwith open(path+ 'val_feature_filenames.pkl', 'rb') as f:\n    val_feature_filenames = pickle.load(f)\n\nprint(len(train_label_filenames), len(val_label_filenames))","metadata":{"id":"Rr6hnSpt8fwE","execution":{"iopub.status.busy":"2024-02-22T07:50:46.539697Z","iopub.execute_input":"2024-02-22T07:50:46.540788Z","iopub.status.idle":"2024-02-22T07:50:46.571683Z","shell.execute_reply.started":"2024-02-22T07:50:46.540750Z","shell.execute_reply":"2024-02-22T07:50:46.570638Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# load pickled mean and standard deviation into lists\n# path = defined previously\nwith open(path + 'mean_ofpixels_perfeature.pkl', 'rb') as f:\n    mean = tf.constant(pickle.load(f))\n\nwith open(path + 'std_ofpixels_perfeature.pkl', 'rb') as f:\n    std_dev = tf.constant(pickle.load(f))\nprint('Mean', mean)\nprint('Variance', std_dev**2)","metadata":{"id":"7BGcvXfY-11I","execution":{"iopub.status.busy":"2024-02-22T07:50:53.726903Z","iopub.execute_input":"2024-02-22T07:50:53.727377Z","iopub.status.idle":"2024-02-22T07:50:55.012586Z","shell.execute_reply.started":"2024-02-22T07:50:53.727345Z","shell.execute_reply":"2024-02-22T07:50:55.011536Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"### LOAD DATA ###\ntrain_image_paths, train_mask_paths = get_image_mask_paths(images_dir, labels_dir, train_feature_filenames, train_label_filenames)\nval_image_paths, val_mask_paths = get_image_mask_paths(images_dir, labels_dir, val_feature_filenames, val_label_filenames)\n\n#Ensuring data is correct\nassert len(train_image_paths) == len(train_mask_paths)\n\n# Creating the dataset\ntrain_dataset = tf.data.Dataset.from_generator(\n    lambda: image_mask_generator(train_image_paths, train_mask_paths, mean, std_dev),\n    output_types={'pixel_values': tf.float32, 'labels': tf.uint8},\n    output_shapes={'pixel_values': (8, image_size, image_size), 'labels': (image_size, image_size)}\n)\nval_dataset = tf.data.Dataset.from_generator(\n    lambda: image_mask_generator(val_image_paths, val_mask_paths, mean, std_dev),\n    output_types={'pixel_values': tf.float32, 'labels': tf.uint8},\n    output_shapes={'pixel_values': (8, image_size, image_size), 'labels': (image_size, image_size)}\n)\n\ntrain_dataset = prepare_for_training(train_dataset)\nval_dataset = prepare_for_training(val_dataset)\n\n# # Prefetch for performance optimization\ntrain_dataset = train_dataset.prefetch(auto)\nval_dataset = val_dataset.prefetch(auto)","metadata":{"id":"r8fyIkHl9X_8","execution":{"iopub.status.busy":"2024-02-20T11:48:17.711425Z","iopub.execute_input":"2024-02-20T11:48:17.711791Z","iopub.status.idle":"2024-02-20T11:48:17.786477Z","shell.execute_reply.started":"2024-02-20T11:48:17.711751Z","shell.execute_reply":"2024-02-20T11:48:17.785762Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for i,j in enumerate(train_dataset):\n    #plt.imshow(j['pixel_values'][1][8])\n    print(np.max(j['pixel_values'][1][7]))\n    if i==20:\n        break # this is an infinite dataset to stop manually\n#     plt.show()\n# verify if values mke sense - change band numbers - expected values?","metadata":{"execution":{"iopub.status.busy":"2024-02-20T11:48:28.872887Z","iopub.execute_input":"2024-02-20T11:48:28.873247Z","iopub.status.idle":"2024-02-20T11:48:44.034332Z","shell.execute_reply.started":"2024-02-20T11:48:28.873217Z","shell.execute_reply":"2024-02-20T11:48:44.033373Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# for i in train_dataset:\n#     print(i['pixel_values'])\n#     break","metadata":{"id":"aH_Tn20ratAk","_kg_hide-output":true,"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"steps_per_epoch = len(train_feature_filenames)// batch_size\nval_steps_per_epoch =  len(val_feature_filenames)// batch_size\n\nprint(f\"Shape of training dataset:{train_dataset.element_spec}\")\nprint(f\"Shape of test dataset: {val_dataset.element_spec}\")","metadata":{"id":"UU7WIvVE9nEN","outputId":"82b0caf2-6960-4eca-cb54-eca4d2b05765","execution":{"iopub.status.busy":"2024-02-20T02:45:25.114932Z","iopub.execute_input":"2024-02-20T02:45:25.115777Z","iopub.status.idle":"2024-02-20T02:45:25.121226Z","shell.execute_reply.started":"2024-02-20T02:45:25.115746Z","shell.execute_reply":"2024-02-20T02:45:25.120159Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"####################\n#### MODEL TIME ####\n####################\n\nmodel_checkpoint = \"nvidia/mit-b0\"\nid2label = {0: \"background\", 1: \"water\"}\nlabel2id = {label: id for id, label in id2label.items()}\nnum_labels = len(id2label)\nmodel = TFSegformerForSemanticSegmentation.from_pretrained(\n    model_checkpoint,\n    num_channels=8,\n    num_labels=num_labels,\n    id2label=id2label,\n    label2id=label2id,\n    ignore_mismatched_sizes=True,\n)\n\ninitial_learning_rate = 0.0001\n# lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n#     initial_learning_rate,\n#     decay_steps=10000,\n#     decay_rate=0.9,\n#     staircase=True)\noptimizer = tf.keras.optimizers.Adam(learning_rate=initial_learning_rate)\nmodel.compile(optimizer=optimizer, run_eagerly=True)","metadata":{"id":"01tjIN9N9n9e","outputId":"c198bb0c-4484-42db-bcf5-01a57dfedd38","execution":{"iopub.status.busy":"2024-02-20T03:00:07.560274Z","iopub.execute_input":"2024-02-20T03:00:07.561089Z","iopub.status.idle":"2024-02-20T03:00:08.416035Z","shell.execute_reply.started":"2024-02-20T03:00:07.561058Z","shell.execute_reply":"2024-02-20T03:00:08.415305Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"### SET UP TRAINING ###\nearly_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=4)\nhistory = model.fit(\n    train_dataset,\n    validation_data=val_dataset,\n    epochs=epochs,\n    steps_per_epoch = steps_per_epoch,\n    validation_steps = val_steps_per_epoch,\n    callbacks=[early_stopping]\n)\n\n# ### PLOTTING ####\n# Extracting the metrics\nloss = history.history['loss']\nval_loss = history.history['val_loss']","metadata":{"id":"yL-V4r9o9_ts","outputId":"024fd3d7-c79c-4f22-c9fe-dc87ec5e0505","execution":{"iopub.status.busy":"2024-02-20T03:00:24.987050Z","iopub.execute_input":"2024-02-20T03:00:24.987465Z","iopub.status.idle":"2024-02-20T03:38:43.061581Z","shell.execute_reply.started":"2024-02-20T03:00:24.987422Z","shell.execute_reply":"2024-02-20T03:38:43.060607Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"### SAVING METRICS ###\nimport os\nmodel_name = f\"sf_20240220_v3\"\nhistory_df = pd.DataFrame(history.history)\n\ndirectory_name = 'models'\noutput_path = '/kaggle/working/'\n\n# Create the new directory\nnew_directory_path = os.path.join(output_path, directory_name)\nos.makedirs(new_directory_path, exist_ok=True)\n\n# Save plots to image & history to CSV\nhistory_df.to_csv(f'/kaggle/working/models/{model_name}.csv', index=False)\n\n# #save model\nhf_model_name = \"rparasa/\" + model_name\nhf_token = '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'\nmodel.push_to_hub(hf_model_name, token=hf_token)","metadata":{"id":"7D9Nrs5h-Qfp","outputId":"bc9079da-4b35-46a2-ef68-2e1c57123a70","execution":{"iopub.status.busy":"2024-02-20T01:53:30.720707Z","iopub.execute_input":"2024-02-20T01:53:30.721472Z","iopub.status.idle":"2024-02-20T01:53:34.282513Z","shell.execute_reply.started":"2024-02-20T01:53:30.721434Z","shell.execute_reply":"2024-02-20T01:53:34.281618Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"epochs =  # change this based on early stop epoch\nepochs_range = range(epochs)  # 'epochs' is the variable used in 'model.fit()'\nplt.figure(figsize=(6, 6))\nplt.plot(epochs_range, loss, label='Training Loss')\nplt.plot(epochs_range, val_loss, label='Validation Loss')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.title('Training and Validation Loss')\nplt.legend(loc='upper right')\n\nplt.tight_layout()\nplt.savefig(f'/kaggle/working/models/{model_name}.png')","metadata":{"id":"fDc_0Mhz-Jsn","outputId":"bcda7382-2cd8-4a1e-8920-ae39c9c79688","execution":{"iopub.status.busy":"2024-02-20T01:54:14.040446Z","iopub.execute_input":"2024-02-20T01:54:14.041534Z","iopub.status.idle":"2024-02-20T01:54:14.433531Z","shell.execute_reply.started":"2024-02-20T01:54:14.041498Z","shell.execute_reply":"2024-02-20T01:54:14.432596Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Inference","metadata":{}},{"cell_type":"code","source":"#inference\nmodel_checkpoint = \"rparasa/\" +'sf_20240220_v2'\nmodel = TFSegformerForSemanticSegmentation.from_pretrained(model_checkpoint)","metadata":{"id":"XX5KNQ2m-SdK","execution":{"iopub.status.busy":"2024-02-22T07:52:15.162761Z","iopub.execute_input":"2024-02-22T07:52:15.163507Z","iopub.status.idle":"2024-02-22T07:52:20.140043Z","shell.execute_reply.started":"2024-02-22T07:52:15.163474Z","shell.execute_reply":"2024-02-22T07:52:20.138911Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def parse_image(file_path):\n    image = tf.convert_to_tensor(np.array(tifffile.imread(file_path)))\n    return image\n\ndef test_load_and_preprocessdata(image_path, mean, std_dev): #, \n    image = parse_image(image_path)\n    image = tf.transpose(image, (2, 0, 1))  # Adjust dimensions if necessary\n\n    # preprocessing pipeline\n    image = np.concatenate([image, np.zeros_like(image[:3, :, :])], axis=0)\n    image = calc_vv_vh(image)    #add vv/vh\n    image = calc_dem_lc(image)  #add dem + lc\n    image = calc_dist_transform(image)    #add distance transform\n    image = calc_norm(image, mean, variance=std_dev**2, axis=0)   #normalize image\n\n    #drop features not required\n    channels_to_keep = [0,1,2,3,5,6,7,8] \n    modified_image = tf.gather(image, channels_to_keep, axis=0)\n    return {\"pixel_values\": modified_image}\n\ndef calc_vv_vh(image):\n    denominator = image[1, :, :]\n    image[6, :, :] = np.nan_to_num(np.divide(image[0, :, :], denominator), nan=0)\n    return image\n\ndef calc_dem_lc(image):\n    dem_quantile = np.where(image[2,:,:] <= np.quantile(image[2,:,:],0.1), 1, 0)\n    binary_image = np.where(image[4,:,:] >= 80, 1, 0)\n    image[7, :, :] = np.sum([dem_quantile, binary_image], axis=0)\n    return image\n\ndef calc_dist_transform(image):\n    #print(np.min(image[4, :, :]),np.max(image[4, :, :]) )\n    binary_image = np.where(image[4, :, :] >= 80, 0,1).astype(np.uint8)\n    #print(np.min(binary_image), np.max(binary_image))\n    dist = cv2.distanceTransform(binary_image, cv2.DIST_L2, 3)\n    image[8, :, :] = dist\n    return image\n\ndef calc_norm(image, mean, variance, axis):\n    layer = tf.keras.layers.Normalization(axis=axis, mean=mean, variance=variance, invert=False)\n    image = layer(image)\n    return image\n\n# def get_image_mask_paths(images_dir, labels_dir):\n#     image_paths = sorted([os.path.join(images_dir, fname) for fname in sorted(os.listdir(images_dir))])\n#     mask_paths = sorted([os.path.join(labels_dir, fname) for fname in sorted(os.listdir(labels_dir))])\n#     return image_paths, mask_paths\n\ndef test_get_image_mask_paths(images_dir):\n    fnames = sorted(os.listdir(images_dir))\n    image_paths = sorted([os.path.join(images_dir, fname) for fname in fnames])\n    return image_paths, fnames\n\n# def image_mask_generator(image_paths, mask_paths):\n#     for img_path, mask_path in zip(image_paths, mask_paths):\n#         yield load_data(img_path, mask_path)\n\ndef test_image_mask_generator(image_paths, mean, std_dev):\n    for img_path in image_paths:\n        yield test_load_and_preprocessdata(img_path, mean, std_dev)\n\ndef test_prepare_for_training(ds, cache=True):\n    if cache:\n        if isinstance(cache, str):\n            ds = ds.cache(cache)\n        else:\n            ds = ds.cache()\n\n    ds = ds.batch(batch_size)\n    ds = ds.prefetch(buffer_size=auto)\n    return ds\n\n# wrap up upsampling and argmax in a function\ndef upsampled_and_argmax(predictions):\n    upsampled = tf.keras.layers.UpSampling2D(size=(4,4), data_format=\"channels_first\",\n                                           interpolation='bilinear')(predictions.logits)\n    labels = np.argmax(upsampled, axis=1)\n    return labels\n\n# save in files\ndef save_numpy_arrays_as_images(numpy_arrays, target_folder, file_names):\n    # Create the target folder if it doesn't exist\n    os.makedirs(target_folder, exist_ok=True)\n    for i, array in enumerate(numpy_arrays):\n        array = array.astype(np.uint8)\n        image = Image.fromarray(array)\n\n        # Save the image to the target folder with a filename based on the index\n        new_fl = file_names[i].replace(\".tif\", \".png\")\n        image_path = os.path.join(target_folder, new_fl)\n        image.save(image_path)\n    print('Inference images saved successfully.')\n    #return None","metadata":{"execution":{"iopub.status.busy":"2024-02-22T07:57:29.456907Z","iopub.execute_input":"2024-02-22T07:57:29.457834Z","iopub.status.idle":"2024-02-22T07:57:29.477577Z","shell.execute_reply.started":"2024-02-22T07:57:29.457798Z","shell.execute_reply":"2024-02-22T07:57:29.476460Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"### LOAD DATA ###\ntest_images_dir = r\"/kaggle/input/track1/Track1/val/images/\"\ntest_image_paths, file_names = test_get_image_mask_paths(test_images_dir)\n\n# Creating the dataset\ntest_dataset = tf.data.Dataset.from_generator(\n    lambda: test_image_mask_generator(test_image_paths, mean, std_dev),\n    output_types={'pixel_values': tf.float32},\n    output_shapes={'pixel_values': (8, image_size, image_size)}\n)\n\ntest_dataset = test_prepare_for_training(test_dataset, cache = False)\n\n# # Prefetch for performance optimization\ntest_dataset = test_dataset.prefetch(auto)\nnum_steps = len(test_image_paths) // batch_size  # Assuming batch_size is defined\ntest_num_steps = num_steps + 1 if len(test_image_paths) % batch_size != 0 else num_steps","metadata":{"execution":{"iopub.status.busy":"2024-02-22T07:57:40.153421Z","iopub.execute_input":"2024-02-22T07:57:40.153794Z","iopub.status.idle":"2024-02-22T07:57:40.387680Z","shell.execute_reply.started":"2024-02-22T07:57:40.153764Z","shell.execute_reply":"2024-02-22T07:57:40.386762Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"### LOAD DATA ###\n#test_images_dir = r\"/kaggle/input/track1/Track1/val/images/\"\n# for one image\ntest_image_paths, file_names = [r\"/kaggle/input/track1/Track1/train/images/1001.tif\"], [\"1001.tif\"]\n\n# Creating the dataset\ntest_dataset = tf.data.Dataset.from_generator(\n    lambda: test_image_mask_generator(test_image_paths, mean, std_dev),\n    output_types={'pixel_values': tf.float32},\n    output_shapes={'pixel_values': (8, image_size, image_size)}\n)\n\ntest_dataset = test_prepare_for_training(test_dataset, cache = False)\n\n# # Prefetch for performance optimization\ntest_dataset = test_dataset.prefetch(auto)\nnum_steps = len(test_image_paths) // batch_size  # Assuming batch_size is defined\ntest_num_steps = num_steps + 1 if len(test_image_paths) % batch_size != 0 else num_steps","metadata":{"execution":{"iopub.status.busy":"2024-02-22T08:00:28.652237Z","iopub.execute_input":"2024-02-22T08:00:28.652656Z","iopub.status.idle":"2024-02-22T08:00:28.690140Z","shell.execute_reply.started":"2024-02-22T08:00:28.652625Z","shell.execute_reply":"2024-02-22T08:00:28.689399Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_dataset.element_spec","metadata":{"execution":{"iopub.status.busy":"2024-02-20T02:11:44.933271Z","iopub.execute_input":"2024-02-20T02:11:44.934042Z","iopub.status.idle":"2024-02-20T02:11:44.939856Z","shell.execute_reply.started":"2024-02-20T02:11:44.934010Z","shell.execute_reply":"2024-02-20T02:11:44.938922Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_dataset.element_spec","metadata":{"execution":{"iopub.status.busy":"2024-02-22T07:58:58.190857Z","iopub.execute_input":"2024-02-22T07:58:58.191561Z","iopub.status.idle":"2024-02-22T07:58:58.197502Z","shell.execute_reply.started":"2024-02-22T07:58:58.191526Z","shell.execute_reply":"2024-02-22T07:58:58.196515Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predictions = model.predict(test_dataset, steps =test_num_steps)\nlabels = upsampled_and_argmax(predictions)\nprint(len(predictions.logits)) # should be 349\nprint(len(labels))","metadata":{"execution":{"iopub.status.busy":"2024-02-20T02:11:47.494886Z","iopub.execute_input":"2024-02-20T02:11:47.495505Z","iopub.status.idle":"2024-02-20T02:12:16.105481Z","shell.execute_reply.started":"2024-02-20T02:11:47.495474Z","shell.execute_reply":"2024-02-20T02:12:16.104486Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predictions = model.predict(test_dataset, steps =test_num_steps)\nlabels = upsampled_and_argmax(predictions)\nprint(len(predictions.logits))  #one image\nprint(len(labels))","metadata":{"execution":{"iopub.status.busy":"2024-02-22T08:00:41.326306Z","iopub.execute_input":"2024-02-22T08:00:41.326824Z","iopub.status.idle":"2024-02-22T08:00:44.880966Z","shell.execute_reply.started":"2024-02-22T08:00:41.326785Z","shell.execute_reply":"2024-02-22T08:00:44.879900Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.imshow(labels[0])","metadata":{"execution":{"iopub.status.busy":"2024-02-22T08:04:48.439740Z","iopub.execute_input":"2024-02-22T08:04:48.440112Z","iopub.status.idle":"2024-02-22T08:04:48.677216Z","shell.execute_reply.started":"2024-02-22T08:04:48.440084Z","shell.execute_reply":"2024-02-22T08:04:48.676096Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"save_numpy_arrays_as_images(labels,r\"/kaggle/working/segformer_val_inferences_sf_20240220_v3\", file_names)","metadata":{"execution":{"iopub.status.busy":"2024-02-20T02:17:06.565363Z","iopub.execute_input":"2024-02-20T02:17:06.565751Z","iopub.status.idle":"2024-02-20T02:17:07.821027Z","shell.execute_reply.started":"2024-02-20T02:17:06.565710Z","shell.execute_reply":"2024-02-20T02:17:07.819876Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"save_numpy_arrays_as_images(labels,r\"/kaggle/working/segformer_1001_inferences_sf_20240220_v2\", file_names)","metadata":{"execution":{"iopub.status.busy":"2024-02-22T08:01:22.237691Z","iopub.execute_input":"2024-02-22T08:01:22.238365Z","iopub.status.idle":"2024-02-22T08:01:22.256648Z","shell.execute_reply.started":"2024-02-22T08:01:22.238333Z","shell.execute_reply":"2024-02-22T08:01:22.255722Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Create new directory\n# import os\n# directory_name = 'models'\n\n# # Path to the Kaggle output folder\n# output_path = '/kaggle/working/'\n\n# # Create the new directory\n# new_directory_path = os.path.join(output_path, directory_name)\n# os.makedirs(new_directory_path, exist_ok=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import shutil\n# shutil.rmtree(\"/kaggle/working/segformer_val_inferences_sf_20240219_v1\")","metadata":{},"execution_count":null,"outputs":[]}]}